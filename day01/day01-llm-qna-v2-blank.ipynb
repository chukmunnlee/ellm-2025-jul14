{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 1 - Question and Answers\n",
    "In this workshop, you will learning how to write prompts and feed them into LLMs. You\n",
    "will also be learning how to use different prompt techniques to improve the response\n",
    "from the LLM.\n",
    "\n",
    "## Loading and Explorng the Dataset\n",
    "The workshop will be using [`facebook/ExploreToM`](https://huggingface.co/datasets/facebook/ExploreToM) dataset from [HuggingFace](https://huggingface.co)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the following libraries: datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset name\n",
    "dataset_name = \"facebook/ExploreToM\"\n",
    " \n",
    "# TODO: load and explore the dataset\n",
    "ds = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': (13309, 18)}\n",
      "dict_keys(['train'])\n",
      "{'story_structure': Value('string'), 'infilled_story': Value('string'), 'question': Value('string'), 'expected_answer': Value('string'), 'qprop=params': Value('string'), 'qprop=nth_order': Value('int64'), 'qprop=non_unique_mental_state': Value('bool'), 'sprop=is_false_belief_story_1st': Value('bool'), 'sprop=is_false_belief_story_1st_and_2nd': Value('bool'), 'sprop=story_accuracy_1st_raw': Value('float64'), 'sprop=story_accuracy_1st_infilled': Value('float64'), 'sprop=global_idx': Value('int64'), 'param=story_type': Value('string'), 'param=num_stories_total': Value('int64'), 'param=max_sentences': Value('int64'), 'param=num_people': Value('int64'), 'param=num_moves': Value('int64'), 'param=num_rooms': Value('int64')}\n",
      "key: story_structure, value: Dylan entered the green room. Dylan moved the pocket watch to the wooden chest, which is also located in the green room. While this action was happening, Clayton witnessed this action in secret (and only this action). Clayton entered the green room. Dylan moved the pocket watch to the leather satchel, which is also located in the green room.\n",
      "key: infilled_story, value: The bustling theater was a hive of activity on this chilly autumn evening, its worn wooden floors and faded velvet curtains a testament to years of countless performances. The dimly lit green room, tucked away backstage, was a cozy refuge from the chaos, its plush armchairs and ornate wooden chest offering a warm respite for those who needed it. Dylan slipped away from the crowd, disappearing behind a tattered curtain as he made his way to a more secluded space. The soft glow of a floor lamp in the green room enveloped him, providing a sense of tranquility amidst the pre-show chaos. Dylan carefully placed the pocket watch in the ornate wooden chest, hidden from view, and Clayton caught a glimpse of this sneaky maneuver from his secret vantage point. Clayton's eyes narrowed as he pushed open the creaky door and stepped into the green room, the sudden movement making the softly lit space seem almost anticipatory. In a swift motion, Dylan delved into the leather satchel, repositioning its contents to accommodate the pocket watch, which now nestled among its weathered confines in the green room.\n",
      "key: question, value: In which container was the pocket watch at the beginning?\n",
      "key: expected_answer, value: wooden chest\n",
      "key: qprop=params, value: (None, 'pocket watch', 'memory-container_location')\n",
      "key: qprop=nth_order, value: -1\n",
      "key: qprop=non_unique_mental_state, value: True\n",
      "key: sprop=is_false_belief_story_1st, value: False\n",
      "key: sprop=is_false_belief_story_1st_and_2nd, value: False\n",
      "key: sprop=story_accuracy_1st_raw, value: 0.0\n",
      "key: sprop=story_accuracy_1st_infilled, value: 1.0\n",
      "key: sprop=global_idx, value: 1\n",
      "key: param=story_type, value: tomi+object-state+asymmetric\n",
      "key: param=num_stories_total, value: 10\n",
      "key: param=max_sentences, value: 15\n",
      "key: param=num_people, value: 2\n",
      "key: param=num_moves, value: 2\n",
      "key: param=num_rooms, value: 1\n"
     ]
    }
   ],
   "source": [
    "# TODO: number of rows in the dataset\n",
    "print(ds.shape)\n",
    "\n",
    "# TODO: Keys in the dataset\n",
    "print(ds.keys())\n",
    "\n",
    "# TODO: Feature names\n",
    "print(ds['train'].features)\n",
    "\n",
    "# TODO: Display a single row\n",
    "idx = 10\n",
    "for k, v in ds['train'][idx].items():\n",
    "   print(f\"key: {k}, value: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: import pipeline\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pipeline`\n",
    "[`pipeline`](https://huggingface.co/docs/transformers/en/main_classes/pipelines) is an easy to use API to perform inferencing. It provides a wrapper for task-specific pipelines and abstracts most of the complexity by allowing you to focus on the model and the task. \n",
    "\n",
    "You can use `pipeline` to perform summarisation, image classification, audio generation, etc. You can find an exhaustive list of `pipeline` task [here](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.pipeline.task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# TODO: Summarise the text with the pipeline's default model\n",
    "qna = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 100\n",
    "story = ds['train'][idx]['story_structure']\n",
    "story_infilled = ds['train'][idx]['infilled_story']\n",
    "question = ds['train'][idx]['question']\n",
    "expected_answer = ds['train'][idx]['expected_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: What does Isabella think about Colton's belief on festival marketing strategies? (knows about it / does not know about it)\n",
      "story: Leslie entered the main tent. Leslie left the main tent. Isabella entered the storage trailer. Isabella moved the stuffed rabbit to the wooden chest, which is also located in the storage trailer. Leslie entered the main tent. Isabella moved the stuffed rabbit to the main tent, leaving the wooden chest in its original location. Isabella told out loud about the festival marketing strategies. Isabella told privately to Colton that Leslie is in the main tent. While this action was happening, Leslie witnessed this action in secret (and only this action).\n",
      "predicted: {'score': 0.20097941160202026, 'start': 1739, 'end': 1765, 'answer': 'Isabella mouthed a message'}\n",
      "actual: does not know about it\n"
     ]
    }
   ],
   "source": [
    "# question = the question we are asking\n",
    "# context = story\n",
    "result = qna(question=question, context=story_infilled)\n",
    "\n",
    "print(f'question: {question}')\n",
    "print(f'story: {story}')\n",
    "print(f'predicted: {result}')\n",
    "print(f'actual: {expected_answer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Inference - Question and Answer\n",
    "In this section, we will look at what `pipeline` does under the hood to perform its inference. This will give us a better understanding of the major steps involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load tokenizer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT base cased distilled SQuAD\n",
    "DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. More details [here](https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert/distilbert-base-cased-distilled-squad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1992,  1602, 15430, 24752,  1116,  1602,  1892,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Encode text\n",
    "message = \"big black bug bleeds black blood\"\n",
    "#message = \"A survey of PostgreSQL users has found that the levels of uptime experienced using cloud providers falls well short of their expectations in terms of reliability.\"\n",
    "\n",
    "# encode the text\n",
    "enc_text = tokenizer(message, return_tensors='pt')\n",
    "\n",
    "print(enc_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(101) [CLS]\n",
      "tensor(1992) big\n",
      "tensor(1602) black\n",
      "tensor(15430) bug\n",
      "tensor(24752) bleed\n",
      "tensor(1116) ##s\n",
      "tensor(1602) black\n",
      "tensor(1892) blood\n",
      "tensor(102) [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i in enc_text.input_ids[0]:\n",
    "   print(i, tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1992,  1602, 15430, 24752,  1116,  1602,  1892,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1131,  4582,  2343, 11251,  1113,  1103,  2343,  5781,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   138,  1263,  1159,  2403,   117,  1107,   170, 15593,  1677,\n",
      "          1677,  1283,   102,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   157, 14640,   188, 24537,  1136,  1294,   170,  3395,  1107,\n",
      "          1103,  1176,  1757,  1104,   170,  1769,  1713,   119,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Encoding multiple texts\n",
    "messages = [\n",
    "   'big black bug bleeds black blood',\n",
    "   'she sell sea shells on the sea shore',\n",
    "   'A long time ago, in a galaxy far far away',\n",
    "   'Thou shalt not make a machine in the likeness of a human mind.'\n",
    "]\n",
    "\n",
    "enc_texts = tokenizer(messages, return_tensors='pt', padding=True)\n",
    "\n",
    "print(enc_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thou shalt not make a machine in the likeness of a human mind.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Decode text\n",
    "print(tokenizer.decode(enc_texts.input_ids[3], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1327,  1674, 10206,  1341,  1164, 23065,   112,   188,  6369,\n",
      "          1113,  3782,  6213, 10700,   136,   113,  3520,  1164,  1122,   120,\n",
      "          1674,  1136,  1221,  1164,  1122,   114,   102]])\n",
      "tensor([[  101,  8521,  2242,  1103,  1514,  9459,   119,  8521,  1286,  1103,\n",
      "          1514,  9459,   119, 10206,  2242,  1103,  5092,  9404,   119, 10206,\n",
      "          1427,  1103, 12084, 16225,  1106,  1103,  4122,  2229,   117,  1134,\n",
      "          1110,  1145,  1388,  1107,  1103,  5092,  9404,   119,  8521,  2242,\n",
      "          1103,  1514,  9459,   119, 10206,  1427,  1103, 12084, 16225,  1106,\n",
      "          1103,  1514,  9459,   117,  2128,  1103,  4122,  2229,  1107,  1157,\n",
      "          1560,  2450,   119, 10206,  1500,  1149,  4632,  1164,  1103,  3782,\n",
      "          6213, 10700,   119, 10206,  1500,  9045,  1106, 23065,  1115,  8521,\n",
      "          1110,  1107,  1103,  1514,  9459,   119,  1799,  1142,  2168,  1108,\n",
      "          5664,   117,  8521,  9491,  1142,  2168,  1107,  3318,   113,  1105,\n",
      "          1178,  1142,  2168,   114,   119,   102]])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the question and the story\n",
    "enc_question = tokenizer(question, return_tensors='pt').input_ids\n",
    "enc_story = tokenizer(story, return_tensors='pt').input_ids\n",
    "\n",
    "print(enc_question)\n",
    "print(enc_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with LLMs\n",
    "Create and instance of the Large Language Model (LLM). We will then create a simple\n",
    "prompt, tokenize the prompt and feed the tokenized prompt to the LLM. The response\n",
    "from the LLM will be decoded to human friendly text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load libraries\n",
    "from transformers import AutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load question answer model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Encode context and question\n",
    "# see above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1327,  1674, 10206,  1341,  1164, 23065,   112,   188,  6369,\n",
      "          1113,  3782,  6213, 10700,   136,   113,  3520,  1164,  1122,   120,\n",
      "          1674,  1136,  1221,  1164,  1122,   114,   102,  8521,  2242,  1103,\n",
      "          1514,  9459,   119,  8521,  1286,  1103,  1514,  9459,   119, 10206,\n",
      "          2242,  1103,  5092,  9404,   119, 10206,  1427,  1103, 12084, 16225,\n",
      "          1106,  1103,  4122,  2229,   117,  1134,  1110,  1145,  1388,  1107,\n",
      "          1103,  5092,  9404,   119,  8521,  2242,  1103,  1514,  9459,   119,\n",
      "         10206,  1427,  1103, 12084, 16225,  1106,  1103,  1514,  9459,   117,\n",
      "          2128,  1103,  4122,  2229,  1107,  1157,  1560,  2450,   119, 10206,\n",
      "          1500,  1149,  4632,  1164,  1103,  3782,  6213, 10700,   119, 10206,\n",
      "          1500,  9045,  1106, 23065,  1115,  8521,  1110,  1107,  1103,  1514,\n",
      "          9459,   119,  1799,  1142,  2168,  1108,  5664,   117,  8521,  9491,\n",
      "          1142,  2168,  1107,  3318,   113,  1105,  1178,  1142,  2168,   114,\n",
      "           119,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Tokenize the inputs\n",
    "inputs = tokenizer(question, story, return_tensors='pt', padding=True)\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure minimum and maximum token length in the answer\n",
    "def ensure_size(input_ids, answer, min_length = 2, max_length = 5):\n",
    "   ans_start = torch.argmax(answer['start_logits'])\n",
    "   ans_end = torch.argmax(answer['end_logits']) + 1\n",
    "   ans_length = ans_end - ans_start\n",
    "   if ans_length < min_length:\n",
    "      ans_end = min(ans_start + min_length, len(input_ids[0]))\n",
    "   elif ans_length > max_length:\n",
    "      ans_end = ans_start + max_length\n",
    "   ans_start = max(0, ans_start)\n",
    "   ans_end = min(len(input_ids[0]), ans_end)\n",
    "   return (ans_start, ans_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Return a minimum of 5 tokens\n",
    "result = model(inputs.input_ids, inputs.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-3.9868, -4.3144, -5.9302, -5.6361, -4.8974, -6.5077, -6.4738, -8.0060,\n",
      "         -7.9933, -5.7121, -7.4480, -6.7496, -7.1584, -7.6161, -5.2428, -1.9160,\n",
      "         -0.5701, -4.5180, -4.3380, -2.8144, -0.8237, -2.0957, -3.7787, -5.4427,\n",
      "         -4.1000, -5.0644, -6.5861, -2.4138, -5.7575, -5.9004, -5.2392, -5.9180,\n",
      "         -6.2209, -2.7861, -5.7684, -6.2824, -5.9315, -6.4277, -6.5801, -3.0058,\n",
      "         -5.6845, -6.0471, -5.0800, -6.3556, -6.6277, -2.7128, -4.6214, -6.0810,\n",
      "         -5.0485, -5.9186, -7.3016, -6.3458, -5.3029, -6.4247, -7.8703, -7.3378,\n",
      "         -8.0731, -7.9633, -7.5573, -7.3166, -6.8668, -5.7800, -6.7769, -6.6220,\n",
      "         -2.9940, -5.8296, -5.9024, -5.5028, -6.2005, -5.8419, -0.3613, -4.1207,\n",
      "         -5.9932, -5.0716, -6.0553, -6.8730, -5.9209, -5.3329, -6.0344, -6.6160,\n",
      "         -3.9760, -5.6062, -5.0829, -6.2641, -6.6954, -6.2566, -6.0579, -6.0710,\n",
      "         -2.7393,  4.9249,  5.7277,  6.2308,  3.2548, -0.4929, -2.2093, -0.7287,\n",
      "         -1.9285, -2.6124, -2.0260,  3.4715,  1.8576,  0.7742, -1.8595, -0.8746,\n",
      "          1.4644,  0.3331, -2.5165, -2.3940, -4.1655, -4.0047, -4.9370, -5.1246,\n",
      "         -2.0905, -3.3859, -3.5961, -6.4968, -5.2952, -4.2789, -1.3237, -3.1384,\n",
      "         -3.6949, -4.6229, -4.4414, -3.9744, -5.2020, -5.9451, -4.7055, -4.0059,\n",
      "         -4.8004, -5.7782, -5.0322, -6.5860]], grad_fn=<CloneBackward0>), end_logits=tensor([[-1.6414e+00, -4.8461e+00, -6.5750e+00, -5.9935e+00, -5.3350e+00,\n",
      "         -6.4543e+00, -6.1403e+00, -7.1859e+00, -6.9202e+00, -5.9508e+00,\n",
      "         -7.6322e+00, -7.2483e+00, -6.6208e+00, -4.7858e+00, -4.6009e+00,\n",
      "         -5.8517e+00, -4.0240e+00, -4.9831e+00, -1.3708e+00, -5.2903e+00,\n",
      "         -6.2280e+00, -2.7413e+00, -1.4957e+00, -3.5134e+00,  8.1423e-02,\n",
      "         -2.1386e+00, -6.7631e+00, -3.5819e+00, -6.1789e+00, -6.0760e+00,\n",
      "         -5.6495e+00, -1.6211e+00, -3.7691e+00, -4.1578e+00, -6.2638e+00,\n",
      "         -6.4275e+00, -6.0355e+00, -2.5946e+00, -4.2622e+00, -4.9213e+00,\n",
      "         -6.7989e+00, -6.6541e+00, -5.7210e+00, -2.8307e+00, -3.9640e+00,\n",
      "         -4.9667e+00, -6.1339e+00, -7.1557e+00, -6.7499e+00, -2.9822e+00,\n",
      "         -6.8429e+00, -6.4736e+00, -5.7212e+00, -3.0460e+00, -4.2981e+00,\n",
      "         -6.8416e+00, -7.7502e+00, -7.2082e+00, -6.9284e+00, -7.6107e+00,\n",
      "         -7.1581e+00, -6.1588e+00, -3.2879e+00, -4.2103e+00, -4.1612e+00,\n",
      "         -6.1085e+00, -6.3840e+00, -6.0114e+00, -2.2457e+00, -3.7090e+00,\n",
      "         -3.8960e+00, -5.9863e+00, -7.7264e+00, -7.0619e+00, -3.0568e+00,\n",
      "         -6.8383e+00, -6.4586e+00, -5.8696e+00, -1.5897e+00, -3.3292e+00,\n",
      "         -6.4504e+00, -7.6133e+00, -6.4766e+00, -3.9110e+00, -6.4316e+00,\n",
      "         -6.2214e+00, -5.5328e+00, -2.1024e+00, -9.9196e-01, -1.6134e-01,\n",
      "         -4.4712e-01, -6.6730e-01,  8.2465e+00,  9.7681e-01, -4.1952e+00,\n",
      "         -2.4395e+00, -2.3407e+00,  1.9628e+00,  2.0546e+00, -1.9988e+00,\n",
      "         -2.9392e+00,  2.8036e+00, -3.7579e+00, -3.0431e-02, -4.9881e+00,\n",
      "         -2.5839e+00, -4.7245e+00, -5.3638e+00, -5.0063e+00, -4.4355e+00,\n",
      "          2.2263e+00,  1.6756e+00, -5.8698e+00, -6.2707e+00, -4.1365e+00,\n",
      "         -6.2283e+00, -7.7052e-01, -3.4296e+00, -3.8699e+00, -4.2909e+00,\n",
      "         -5.3903e+00, -1.5649e+00, -7.0021e+00, -6.6062e-03, -3.4864e+00,\n",
      "         -6.2449e+00, -5.2934e+00, -6.0358e+00, -1.5418e+00, -1.2753e+00,\n",
      "          4.5134e-01, -6.7629e+00]], grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out loud\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# find the start and end position\n",
    "ans_start = torch.argmax(result.start_logits)\n",
    "ans_end = torch.argmax(result.end_logits) + 1\n",
    "\n",
    "# extract the answer\n",
    "enc_answer =inputs.input_ids[0][ans_start: ans_end]\n",
    "\n",
    "# decode it\n",
    "answer = tokenizer.decode(enc_answer)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try this your self\n",
    "\n",
    "context = \"\"\"\n",
    "Dickens wrote A Christmas Carol during a period when the British were exploring and re-evaluating past Christmas traditions, \n",
    "including carols, and newer customs such as cards and Christmas trees. He was influenced by the experiences of his own youth and \n",
    "by the Christmas stories of other authors, including Washington Irving and Douglas Jerrold. Dickens had written three Christmas \n",
    "stories prior to the novella, and was inspired following a visit to the Field Lane Ragged School, one of several establishments for \n",
    "London's street children. The treatment of the poor and the ability of a selfish man to redeem himself by transforming into a more \n",
    "sympathetic character are the key themes of the story. There is discussion among academics as to whether this is a fully secular \n",
    "story or a Christian allegory.\n",
    "\"\"\"\n",
    "\n",
    "question = \"How many Christmas stories has Dickens wrote?\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
